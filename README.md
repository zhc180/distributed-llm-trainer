# Distributed LLM Trainer

A learning project for distributed LLM training, inspired by [Hugging Face's Nanotron](https://github.com/huggingface/nanotron).

## ğŸ¯ Learning Goals

By building this project, you'll understand:
- **Data Parallelism (DP)** - Split batches across GPUs
- **Tensor Parallelism (TP)** - Split model layers across GPUs
- **Pipeline Parallelism (PP)** - Split model stages across GPUs
- **FSDP/ZeRO** - Shard optimizer states and gradients
- **Mixed Precision** - BF16/FP16 training
- **Gradient Checkpointing** - Trade compute for memory

## ğŸ“š Recommended Learning Path

### Week 1: Foundations
1. Read the [Ultrascale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
2. Understand DDP basics with `01_ddp_basics.py`
3. Run single-GPU training first

### Week 2: Advanced Parallelism
1. Implement FSDP with `02_fsdp_training.py`
2. Add mixed precision and gradient checkpointing
3. Benchmark and compare approaches

## ğŸ—ï¸ Project Structure

```
distributed-llm-trainer/
â”œâ”€â”€ README.md
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ small_model.yaml      # GPT-2 124M config
â”‚   â”œâ”€â”€ medium_model.yaml     # GPT-2 355M config
â”‚   â””â”€â”€ fsdp_config.yaml      # FSDP settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gpt.py            # GPT model implementation
â”‚   â”‚   â””â”€â”€ config.py         # Model configuration
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ddp_trainer.py    # Basic DDP trainer
â”‚   â”‚   â”œâ”€â”€ fsdp_trainer.py   # FSDP trainer
â”‚   â”‚   â””â”€â”€ trainer_utils.py  # Shared utilities
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dataloader.py     # Distributed data loading
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging.py        # W&B / TensorBoard logging
â”‚       â””â”€â”€ checkpoint.py     # Distributed checkpointing
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ddp.sh          # Launch DDP training
â”‚   â””â”€â”€ train_fsdp.sh         # Launch FSDP training
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ results.md            # Your benchmark results
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install torch torchvision torchaudio
pip install transformers datasets wandb
pip install flash-attn --no-build-isolation  # Optional, for faster attention
```

### 2. Download Datasets (TinyStories + OpenWebText sample)

Run these commands from `src/data` so the files land in the data folder:

```bash
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
gunzip owt_train.txt.gz
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
```

### 3. Single GPU Training (Start Here!)

```bash
python src/training/ddp_trainer.py --config configs/small_model.yaml
```

### 4. Multi-GPU DDP Training

```bash
torchrun --nproc_per_node=2 src/training/ddp_trainer.py --config configs/small_model.yaml
```

### 5. FSDP Training

```bash
torchrun --nproc_per_node=4 src/training/fsdp_trainer.py --config configs/medium_model.yaml
```

### 6. Choose a Dataset (dummy / tinystories / openwebtext)

```bash
# TinyStories
python src/training/ddp_trainer.py \
  --dataset tinystories \
  --data_path src/data/TinyStoriesV2-GPT4-train.txt \
  --model_size small --max_steps 50 --batch_size 4

# OpenWebText sample
python src/training/ddp_trainer.py \
  --dataset openwebtext \
  --data_path src/data/owt_train.txt \
  --model_size small --max_steps 50 --batch_size 4

# Dummy (default)
python src/training/ddp_trainer.py --dataset dummy --model_size small --max_steps 50
```

### 7. Inference from a Checkpoint

```bash
python src/eval/infer.py \
  --checkpoint checkpoints/final.pt \
  --model_size small \
  --prompt "Once upon a time," \
  --max_new_tokens 50
```

## ğŸ“Š Key Concepts Explained

### Data Parallelism (DDP)
```
GPU 0: Full Model + Batch 0 â†’ Gradients 0 â”€â”
GPU 1: Full Model + Batch 1 â†’ Gradients 1 â”€â”¼â”€â†’ All-Reduce â†’ Update
GPU 2: Full Model + Batch 2 â†’ Gradients 2 â”€â”¤
GPU 3: Full Model + Batch 3 â†’ Gradients 3 â”€â”˜
```

### FSDP (Fully Sharded Data Parallel)
```
GPU 0: Shard 0 of (Model + Optimizer + Gradients)
GPU 1: Shard 1 of (Model + Optimizer + Gradients)
GPU 2: Shard 2 of (Model + Optimizer + Gradients)
GPU 3: Shard 3 of (Model + Optimizer + Gradients)
         â†“ All-Gather before forward/backward
         â†“ Reduce-Scatter after backward
```

### Memory Comparison (GPT-2 355M, FP32)

| Method | Model Memory | Optimizer Memory | Total/GPU |
|--------|-------------|------------------|-----------|
| DDP | 1.4 GB | 2.8 GB | 4.2 GB |
| FSDP (4 GPUs) | 0.35 GB | 0.7 GB | ~1.1 GB |

## ğŸ”§ Configuration

### Model Config (`configs/small_model.yaml`)
```yaml
model:
  vocab_size: 50257
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  max_seq_len: 1024

training:
  batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 6e-4
  max_steps: 10000
  warmup_steps: 1000

distributed:
  backend: "nccl"
  mixed_precision: "bf16"
```

## ğŸ“ˆ Benchmarking

Track these metrics:
- **Throughput**: tokens/second
- **Memory**: peak GPU memory per device
- **Scaling efficiency**: throughput vs. # GPUs

```python
# Example benchmark output
"""
Config: GPT-2 124M, batch_size=8, seq_len=1024
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPUs â”‚ Method â”‚ Tokens/sec â”‚ Memory/GPU â”‚ Efficiency
  1  â”‚ Single â”‚    12,500  â”‚   8.2 GB   â”‚   100%
  2  â”‚ DDP    â”‚    24,100  â”‚   8.2 GB   â”‚    96%
  4  â”‚ DDP    â”‚    46,800  â”‚   8.2 GB   â”‚    94%
  4  â”‚ FSDP   â”‚    44,200  â”‚   3.1 GB   â”‚    88%
"""
```

## ğŸ“– Resources

- [Nanotron GitHub](https://github.com/huggingface/nanotron)
- [Ultrascale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)

## âœ… Project Milestones

- [ ] Single GPU training working
- [ ] DDP training with 2+ GPUs
- [ ] FSDP training implemented
- [ ] Mixed precision (BF16) added
- [ ] Gradient checkpointing added
- [ ] W&B logging integrated
- [ ] Benchmark results documented
- [ ] README with architecture diagrams
